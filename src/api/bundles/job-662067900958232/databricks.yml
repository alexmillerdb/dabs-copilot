bundle:
  name: job-662067900958232

variables:
  project_root:
    description: Base workspace path for code
    default: /Workspace/Repos/your-org/your-repo
  default_storage_location:
    description: External storage or base path for outputs
    default: dbfs:/tmp/job-662067900958232
  catalog:
    description: Unity Catalog name
    default: main
  environment:
    description: Target environment
    default: dev
  compute_profile:
    description: Cluster size profile
    default: small

targets:
  dev:
    default: true
    workspace:
      host: ${workspace.host}
      root_path: /Workspace/Users/${workspace.current_user.userName}/bundles/job-662067900958232-dev
    run_as:
      user_name: ${workspace.current_user.userName}

resources:
  jobs:
    job_662067900958232:
      name: "Job 662067900958232 - ${var.environment}"
      
      job_clusters:
        - job_cluster_key: shared-compute
          new_cluster:
            spark_version: "14.3.x-scala2.12"
            node_type_id: "i3.xlarge"
            num_workers: 2
            spark_conf:
              "spark.databricks.cluster.profile": "singleNode"
              "spark.master": "local[*]"
            custom_tags:
              environment: ${var.environment}
              bundle: job-662067900958232
      
      tasks:
        - task_key: main_task
          job_cluster_key: shared-compute
          notebook_task:
            notebook_path: ${var.project_root}/notebooks/main_notebook
            base_parameters:
              catalog: ${var.catalog}
              environment: ${var.environment}
              storage_location: ${var.default_storage_location}
          libraries:
            - pypi:
                package: "pandas>=1.5.0"
            - pypi:
                package: "numpy>=1.21.0"
          timeout_seconds: 3600
          
        - task_key: data_validation
          job_cluster_key: shared-compute
          depends_on:
            - task_key: main_task
          notebook_task:
            notebook_path: ${var.project_root}/notebooks/data_validation
            base_parameters:
              catalog: ${var.catalog}
              environment: ${var.environment}
          timeout_seconds: 1800
      
      # Schedule configuration (commented out - uncomment if needed)
      # schedule:
      #   quartz_cron_expression: "0 0 9 * * ?"
      #   timezone_id: "America/Los_Angeles"
      #   pause_status: "UNPAUSED"
      
      # Email notifications (commented out - configure as needed)
      # email_notifications:
      #   on_failure:
      #     - your-email@company.com
      #   on_success:
      #     - your-email@company.com
      
      max_concurrent_runs: 1
      timeout_seconds: 14400
      
      tags:
        environment: ${var.environment}
        source: "bundle-migration"
        original_job_id: "662067900958232"

  # Example pipeline configuration (if the job includes DLT pipelines)
  # pipelines:
  #   main_pipeline:
  #     name: "Pipeline from Job 662067900958232 - ${var.environment}"
  #     clusters:
  #       - label: default
  #         num_workers: 1
  #         spark_conf:
  #           "spark.databricks.cluster.profile": "singleNode"
  #     libraries:
  #       - notebook:
  #           path: ${var.project_root}/pipelines/dlt_pipeline
  #     configuration:
  #       pipeline_storage: ${var.default_storage_location}/pipeline
  #     development: true
  #     continuous: false
  #     channel: "CURRENT"

  # Example shared cluster (if needed across multiple resources)
  # clusters:
  #   shared_cluster:
  #     cluster_name: "Shared Cluster - ${var.environment}"
  #     spark_version: "14.3.x-scala2.12"
  #     node_type_id: "i3.xlarge"
  #     num_workers: 2
  #     autotermination_minutes: 30
  #     custom_tags:
  #       environment: ${var.environment}
  #       bundle: job-662067900958232