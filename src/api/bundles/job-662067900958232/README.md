# Job 662067900958232 - Databricks Asset Bundle

This bundle was generated from Databricks job ID `662067900958232` and contains all the necessary configuration to deploy and run the job using Databricks Asset Bundles (DAB).

## Bundle Structure

```
job-662067900958232/
├── databricks.yml          # Main bundle configuration
└── README.md              # This file
```

## Prerequisites

1. **Databricks CLI**: Install and configure the Databricks CLI
   ```bash
   pip install databricks-cli
   databricks configure
   ```

2. **Workspace Authentication**: Ensure you're authenticated to your Databricks workspace
   ```bash
   databricks auth login
   ```

3. **Required Permissions**: You need permissions to:
   - Create and manage jobs
   - Create compute clusters
   - Access the specified notebook paths
   - Write to the configured storage locations

## Configuration

### Variables

The bundle uses the following configurable variables (defined in `databricks.yml`):

| Variable | Description | Default Value |
|----------|-------------|---------------|
| `project_root` | Base workspace path for notebooks | `/Workspace/Repos/your-org/your-repo` |
| `default_storage_location` | Storage path for outputs | `dbfs:/tmp/job-662067900958232` |
| `catalog` | Unity Catalog name | `main` |
| `environment` | Target environment | `dev` |
| `compute_profile` | Cluster size profile | `small` |

### Customization

Before deploying, you should update:

1. **Notebook Paths**: Update the notebook paths in the job tasks to match your workspace structure
2. **Storage Locations**: Configure appropriate storage paths for your environment
3. **Libraries**: Add or modify the Python packages based on your job requirements
4. **Cluster Configuration**: Adjust cluster size and configuration for your workload

## Deployment Commands

### 1. Validate the Bundle
```bash
databricks bundle validate
```

### 2. Deploy to Development Environment
```bash
databricks bundle deploy -t dev
```

### 3. Run the Job
```bash
databricks bundle run job_662067900958232 -t dev
```

### 4. Monitor Job Execution
```bash
databricks bundle run job_662067900958232 -t dev --watch
```

## Job Configuration

### Tasks
- **main_task**: Primary notebook execution with configurable parameters
- **data_validation**: Validation step that runs after the main task completes

### Cluster Configuration
- **Spark Version**: 14.3.x-scala2.12 (Latest LTS)
- **Node Type**: i3.xlarge (balanced compute/memory)
- **Workers**: 2 nodes for development
- **Auto-termination**: Configured for cost optimization

### Libraries
- pandas>=1.5.0
- numpy>=1.21.0

## Environment-Specific Configuration

### Development (dev)
- Single job run capability
- Smaller cluster configuration
- Development-friendly timeout settings

### Adding Production Target
To add a production target, add this section to `databricks.yml`:

```yaml
targets:
  prod:
    workspace:
      host: ${workspace.host}
      root_path: /Workspace/bundles/job-662067900958232-prod
    run_as:
      service_principal_name: your-service-principal@company.com
    variables:
      environment: prod
      compute_profile: large
```

## Troubleshooting

### Common Issues

1. **Authentication Error**: Run `databricks auth login` to re-authenticate
2. **Notebook Not Found**: Update notebook paths in the job tasks
3. **Cluster Start Failure**: Check cluster configuration and permissions
4. **Library Installation Error**: Verify PyPI package versions and compatibility

### Validation Failures

If `databricks bundle validate` fails:
1. Check the error message for specific issues
2. Verify all notebook paths exist in your workspace
3. Ensure storage locations are accessible
4. Confirm cluster configurations are valid for your workspace

### Performance Tuning

For production workloads, consider:
- Increasing cluster size based on data volume
- Adjusting timeout values for long-running tasks
- Configuring auto-scaling clusters
- Adding monitoring and alerting

## Next Steps

1. **Customize Configuration**: Update notebook paths, storage locations, and variables
2. **Test Deployment**: Run validation and deployment in development
3. **Add Monitoring**: Configure email notifications and logging
4. **Production Setup**: Add production target and service principal authentication
5. **CI/CD Integration**: Integrate bundle deployment into your CI/CD pipeline

## Support

For issues with this bundle:
1. Check the Databricks documentation: https://docs.databricks.com/dev-tools/bundles/
2. Validate your configuration with `databricks bundle validate`
3. Review job logs in the Databricks workspace UI

---

*Generated by Databricks Asset Bundle Generator*
*Original Job ID: 662067900958232*